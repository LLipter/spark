#第二次进度汇报

## 第二阶段已完成部分

1. 如何简单的部署spark（了然）
2. 组件之间分层结构关系（肖健）
3. stakeholders，spark的详细介绍和时间年表。（朱昱睿）


##学习感受
### 了然：
我主要研究spark的使用方法以及其背后的原理。
为了找到一种高校的方式处理越来越庞大的数据，很多分布式项目应运而生了，比如MapReduce等等。但这些项目都有种种的劣势。比如在MapReduce中。如果想要开发一个作业，既要写Map,又要写Reduce和驱动类。当需求变动的时候要改变大量的代码，重用性很差。在这种情况下，spark诞生了。Spark最初诞生自伯克利实验室的一片论文————“Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing” 在这篇论文中提出了一种粗粒度的数据结构————RDD，有别于NapReduce中使用的细粒度算法。RDD所能表达的操作虽然不如mapreduce丰富，但是极具伸缩性以及可扩展性。能够轻松的扩展到大型集群上，并且提供了很好的容错机制，能够很好的平衡负载，提高效率。

###朱昱睿：
通过对spark的简介，stakeholder和发展简介的撰写，我开始明白spark是应运而生的。spark通过优化运行速度和兼容性，核心代码简洁，巧妙借用其他的大数据组件，灵活轻便，由于mapreduce，Hadoop等同类数据运算框架。

###肖建：
这周主要完成了组件的分层结构分析以及组件交互方式的了解。通过阅读官方文档，对组件的功能与协作有进一步的了解，清楚了RDD的基本工作模式。最后通过运行Spark和查找资料，完成了对分层结构和交互方式的分析。
##下一阶段目标
1. 继续spark软件的深入了解
2. 继续软件体系结构知识的学习
3. 在现有基础上进一步完善各个部分。
4. 优化sa的体系，使整个说明更有条理性
